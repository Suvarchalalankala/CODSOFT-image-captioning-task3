{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PENmUMfaCP-h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# Load the VGG16 model with pre-trained ImageNet weights\n",
        "model = VGG16(weights='imagenet', include_top=True)\n"
      ],
      "metadata": {
        "id": "UADoWoqhEUjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Open image files\n",
        "img1 = Image.open('/content/image1.jfif')\n",
        "img2 = Image.open('/content/image2.jfif')\n",
        "img3 = Image.open('/content/image3.jfif')\n"
      ],
      "metadata": {
        "id": "LGI9il62HNGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_input(img_array)\n",
        "    return img_array\n"
      ],
      "metadata": {
        "id": "d_d6L-bbEoj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_captions(image_ids, captions):\n",
        "    tokenized_captions = [word_tokenize(caption.lower()) for caption in captions]\n",
        "    return tokenized_captions\n"
      ],
      "metadata": {
        "id": "cQO5P3RZEsie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_captions(captions):\n",
        "    tokenized_captions = [word_tokenize(caption.lower()) for caption in captions]\n",
        "    return tokenized_captions"
      ],
      "metadata": {
        "id": "EwwmkQ1-E0ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary(tokenized_captions):\n",
        "    vocabulary = set()\n",
        "    for caption in tokenized_captions:\n",
        "        vocabulary.update(caption)\n",
        "    return vocabulary\n"
      ],
      "metadata": {
        "id": "1ghFlUrrE2V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_index_mappings(vocabulary):\n",
        "    word_to_index = {word: idx + 1 for idx, word in enumerate(vocabulary)}\n",
        "    index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "    return word_to_index, index_to_word"
      ],
      "metadata": {
        "id": "vBXCDsb_E7WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_image_features(image_path):\n",
        "    img_array = preprocess_image(image_path)\n",
        "    features = model.predict(img_array)\n",
        "    return features"
      ],
      "metadata": {
        "id": "y4QkzZIKFAFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for training\n",
        "def prepare_data(image_ids, captions, max_seq_length, word_to_index):\n",
        "    X1, X2, y = [], [], []\n",
        "    for i in range(len(image_ids)):\n",
        "        for caption in captions[i]:\n",
        "            seq = [word_to_index[word] for word in caption.split() if word in word_to_index]\n",
        "            for j in range(1, len(seq)):\n",
        "                in_seq, out_seq = seq[:j], seq[j]\n",
        "                in_seq = pad_sequences([in_seq], maxlen=max_seq_length)[0]\n",
        "                out_seq = to_categorical([out_seq], num_classes=len(word_to_index) + 1)[0]\n",
        "                X1.append(extract_image_features(f\"images/{image_ids[i]}.jpg\"))\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)"
      ],
      "metadata": {
        "id": "pmnsh_CQFFBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(vocab_size, max_length):\n",
        "    inputs1 = Input(shape=(4096,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "    decoder1 = tf.keras.layers.add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    print(model.summary())\n",
        "    return model"
      ],
      "metadata": {
        "id": "X4NrODVAFK51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids = ['image1', 'image2', 'image3']  # Example list of image IDs\n",
        "captions = [\n",
        "    [\"a black dog is running on the grass\"],  # Example captions for image1\n",
        "    [\"a brown horse is grazing in the field\"],  # Example captions for image2\n",
        "    [\"a person is riding a bicycle on the road\"]  # Example captions for image3\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "akr0dLUzFQjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary(tokenized_captions):\n",
        "    vocabulary = set()\n",
        "    for caption_list in tokenized_captions:\n",
        "        for caption in caption_list:\n",
        "            vocabulary.update(caption)\n",
        "    return vocabulary\n"
      ],
      "metadata": {
        "id": "ZuzDcCYMJrUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_captions(captions):\n",
        "    tokenized_captions = []\n",
        "    for caption_list in captions:\n",
        "        tokenized_caption_list = []\n",
        "        for caption in caption_list:\n",
        "            tokenized_caption_list.append(word_tokenize(caption.lower()))\n",
        "        tokenized_captions.append(tokenized_caption_list)\n",
        "    return tokenized_captions\n"
      ],
      "metadata": {
        "id": "n57iniQGHwxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjEXwaoDIYYQ",
        "outputId": "b0b1e622-f636-461d-da9b-0120da642c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize captions\n",
        "tokenized_captions = tokenize_captions(captions)\n"
      ],
      "metadata": {
        "id": "7S3YSQ9VIMJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize captions\n",
        "tokenized_captions = tokenize_captions(captions)\n",
        "\n",
        "# Create vocabulary\n",
        "vocabulary = create_vocabulary(tokenized_captions)\n",
        "\n",
        "# Map words to indices and vice versa\n",
        "word_to_index, index_to_word = create_word_index_mappings(vocabulary)\n"
      ],
      "metadata": {
        "id": "-l0B-gG4JTkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = max(len(seq) for seq in tokenized_captions)\n"
      ],
      "metadata": {
        "id": "I51-PsDNIa4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for caption in captions:\n",
        "    seq = [word_to_index[word] for word in \" \".join(caption).split() if word in word_to_index]\n",
        "    # Further processing using seq\n"
      ],
      "metadata": {
        "id": "xrrGRPyfKJlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq = [word_to_index[word] for word in \" \".join(caption).split() if word in word_to_index]\n"
      ],
      "metadata": {
        "id": "na9YybCKJ9X_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_model(len(vocabulary) + 1, max_seq_length)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGZ2KXmVKPvP",
        "outputId": "16a3fc5c-58c5-4294-8ff0-a54c123a73fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)       [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_10 (InputLayer)       [(None, 4096)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)     (None, 1, 256)               4608      ['input_11[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 4096)                 0         ['input_10[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)         (None, 1, 256)               0         ['embedding_2[0][0]']         \n",
            "                                                                                                  \n",
            " dense_6 (Dense)             (None, 256)                  1048832   ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)               (None, 256)                  525312    ['dropout_5[0][0]']           \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 256)                  0         ['dense_6[0][0]',             \n",
            "                                                                     'lstm_2[0][0]']              \n",
            "                                                                                                  \n",
            " dense_7 (Dense)             (None, 256)                  65792     ['add_2[0][0]']               \n",
            "                                                                                                  \n",
            " dense_8 (Dense)             (None, 18)                   4626      ['dense_7[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1649170 (6.29 MB)\n",
            "Trainable params: 1649170 (6.29 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify data types\n",
        "print(type(X1), type(X2), type(y))\n",
        "\n",
        "# Verify data shapes if they are not ellipsis objects\n",
        "if not isinstance(X1, type(...)) and not isinstance(X2, type(...)) and not isinstance(y, type(...)):\n",
        "    print(X1.shape, X2.shape, y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtVP-xyHPYjn",
        "outputId": "a6a4318f-6e15-4bc0-eb4e-c0b8c5cf73db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'ellipsis'> <class 'ellipsis'> <class 'ellipsis'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('image_captioning_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GuiTmllOcxe",
        "outputId": "9fe2bd42-0c67-44bb-fb01-597f9256ab54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists('image_captioning_model.h5'):\n",
        "    print(\"Model saved successfully.\")\n",
        "else:\n",
        "    print(\"Error: Model not saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zfGwu6mSjd8",
        "outputId": "fe994b11-f154-4124-a5cc-f097ec8aacca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = load_model('image_captioning_model.h5')\n"
      ],
      "metadata": {
        "id": "hA_H9q3uSlQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display model summary\n",
        "loaded_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0A3NDJ-Spa8",
        "outputId": "6e7c632c-c933-46fa-c6e8-22f45e0c756d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_8 (InputLayer)        [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " input_7 (InputLayer)        [(None, 4096)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 1, 256)               4608      ['input_8[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 4096)                 0         ['input_7[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 1, 256)               0         ['embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 256)                  1048832   ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, 256)                  525312    ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 256)                  0         ['dense_3[0][0]',             \n",
            "                                                                     'lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 256)                  65792     ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " dense_5 (Dense)             (None, 18)                   4626      ['dense_4[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1649170 (6.29 MB)\n",
            "Trainable params: 1649170 (6.29 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}